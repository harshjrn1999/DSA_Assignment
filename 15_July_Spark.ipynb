{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c691914",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Python program to create an RDD from a local data source:\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'RDDExample')\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "b) Transformations and actions on the RDD to perform data processing tasks:\n",
    "\n",
    "# Transformation: Map\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Action: Collect\n",
    "squared_data = squared_rdd.collect()\n",
    "print(squared_data)  # [1, 4, 9, 16, 25]\n",
    "\n",
    "# Transformation: Filter\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Action: Count\n",
    "count = filtered_rdd.count()\n",
    "print(count)  # 2\n",
    "\n",
    "# Transformation: Reduce\n",
    "sum = rdd.reduce(lambda x, y: x + y)\n",
    "print(sum)  # 15\n",
    "c) Analyzing and manipulating data using RDD operations:\n",
    "\n",
    "# Transformation: FlatMap\n",
    "words_rdd = rdd.flatMap(lambda x: str(x).split())\n",
    "words_data = words_rdd.collect()\n",
    "print(words_data)  # ['1', '2', '3', '4', '5']\n",
    "\n",
    "# Transformation: Aggregate\n",
    "product = rdd.aggregate(1, lambda x, y: x * y, lambda x, y: x * y)\n",
    "print(product)  # 120\n",
    "\n",
    "# Transformation: Key-Value RDD\n",
    "data = [('Alice', 25), ('Bob', 30), ('Charlie', 35)]\n",
    "kv_rdd = sc.parallelize(data)\n",
    "filtered_kv_rdd = kv_rdd.filter(lambda x: x[1] >= 30)\n",
    "filtered_kv_data = filtered_kv_rdd.collect()\n",
    "print(filtered_kv_data)  # [('Bob', 30), ('Charlie', 35)]\n",
    "Spark DataFrame Operations:\n",
    "a) Python program to load a CSV file into a Spark DataFrame:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "b) Performing common DataFrame operations:\n",
    "\n",
    "# Filtering\n",
    "filtered_df = df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# Grouping\n",
    "grouped_df = df.groupBy(\"gender\").count()\n",
    "\n",
    "# Joining\n",
    "other_df = spark.read.csv(\"path/to/other_file.csv\", header=True, inferSchema=True)\n",
    "joined_df = df.join(other_df, on=\"id\", how=\"inner\")\n",
    "c) Applying Spark SQL queries on the DataFrame:\n",
    "\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Spark SQL query\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "result.show()\n",
    "Spark Streaming:\n",
    "a) Python program to create a Spark Streaming application:\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, batchDuration=1)  # batch duration in seconds\n",
    "b) Configuring the application to consume data from a streaming source:\n",
    "\n",
    "# Consume from Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "kafka_params = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"my_consumer_group\"\n",
    "}\n",
    "topic = \"my_topic\"\n",
    "stream = KafkaUtils.createDirectStream(ssc, [topic], kafka_params)\n",
    "c) Implementing streaming transformations and actions:\n",
    "\n",
    "# Streaming transformation: Map\n",
    "stream_data = stream.map(lambda x: x[1])\n",
    "\n",
    "# Streaming action: Print first 10 records in each batch\n",
    "stream_data.pprint(num=10)\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "Spark SQL and Data Source Integration:\n",
    "a) Python program to connect Spark with a relational database:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLExample\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/mysql-connector-java.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "table_name = \"mytable\"\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", table_name).load()\n",
    "b) Performing SQL operations on the data stored in the database using Spark SQL:\n",
    "\n",
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "# Spark SQL query\n",
    "result = spark.sql(\"SELECT * FROM mytable WHERE age > 30\")\n",
    "result.show()\n",
    "c) Exploring the integration capabilities of Spark with other data sources:\n",
    "\n",
    "# Reading from HDFS\n",
    "hdfs_path = \"/path/to/data.csv\"\n",
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Reading from Amazon S3\n",
    "s3_path = \"s3a://my-bucket/data.csv\"\n",
    "df = spark.read.csv(s3_path, header=True, inferSchema=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
